{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOsiAE10Up/U2JjiKtgU0tG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/za4n/A-budgetApp/blob/master/vtryModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0R3vEnANZCau",
        "outputId": "90d83dd9-bf1a-4780-f881-43db9de64ce0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.17)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.6)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.2.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.10)\n"
          ]
        }
      ],
      "source": [
        "# Install kaggle package\n",
        "!pip install kaggle\n",
        "\n",
        "# Create directory for Kaggle API token\n",
        "!mkdir -p ~/.kaggle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Move the Kaggle API token to the correct location\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "# Set correct permissions\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "-oBLjMfkZZLw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create a project directory in your Google Drive\n",
        "!mkdir -p \"/content/drive/My Drive/VITON_HD_Project\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irSVdKSXZecP",
        "outputId": "c22affac-363f-4214-b0df-526836e8b7e2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create necessary subdirectories\n",
        "!mkdir -p \"/content/drive/My Drive/VITON_HD_Project/dataset\"\n",
        "!mkdir -p \"/content/drive/My Drive/VITON_HD_Project/models\"\n",
        "!mkdir -p \"/content/drive/My Drive/VITON_HD_Project/results\""
      ],
      "metadata": {
        "id": "HbJMXGmxZr5Z"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change to the dataset directory\n",
        "%cd \"/content/drive/My Drive/VITON_HD_Project/dataset\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJI-cHHfZvn-",
        "outputId": "0c6b5081-26c6-4323-f571-1234b1cbb778"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/VITON_HD_Project/dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the dataset\n",
        "!kaggle datasets download marquis03/high-resolution-viton-zalando-dataset\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7xfTSMjZ1r0",
        "outputId": "0e98d853-cd6f-4ef6-c2e4-bc01d0166087"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/marquis03/high-resolution-viton-zalando-dataset\n",
            "License(s): CC-BY-NC-SA-4.0\n",
            "Downloading high-resolution-viton-zalando-dataset.zip to /content/drive/My Drive/VITON_HD_Project/dataset\n",
            "100% 4.38G/4.39G [01:03<00:00, 98.3MB/s]\n",
            "100% 4.39G/4.39G [01:03<00:00, 73.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q high-resolution-viton-zalando-dataset.zip"
      ],
      "metadata": {
        "id": "PiHjjGPuaNHb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if GPU is being recognized\n",
        "import torch\n",
        "print(\"GPU Available:\", torch.cuda.is_available())\n",
        "print(\"GPU Device Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RicsrtivfED0",
        "outputId": "24f39945-5f7e-4256-c562-e689707c4502"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Available: True\n",
            "GPU Device Name: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision\n",
        "!pip install opencv-python-headless\n",
        "!pip install numpy pandas matplotlib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1IlKah8efJLO",
        "outputId": "9f907a2d-d037-47f3-ae74-57f694e1e6ab"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python-headless) (1.26.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Verify GPU\n",
        "print(\"GPU Available:\", torch.cuda.is_available())\n",
        "print(\"GPU Device Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")\n",
        "\n",
        "# Check dataset structure\n",
        "base_path = '/content/drive/MyDrive/VITON_HD_Project/dataset'\n",
        "print(\"\\nDataset contents:\")\n",
        "print(os.listdir(base_path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNmNN--_fXDj",
        "outputId": "2d85988f-6e88-4e4e-d0a6-a918cae6afe3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Available: True\n",
            "GPU Device Name: Tesla T4\n",
            "\n",
            "Dataset contents:\n",
            "['high-resolution-viton-zalando-dataset.zip', 'test', 'test_pairs.txt', 'train', 'train_pairs.txt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read and display first few pairs from test_pairs.txt\n",
        "pairs_file = os.path.join(base_path, 'test_pairs.txt')\n",
        "print(\"First 5 lines of test_pairs.txt:\")\n",
        "with open(pairs_file, 'r') as f:\n",
        "    for i, line in enumerate(f):\n",
        "        if i < 5:\n",
        "            print(line.strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2A2j3nAf5JT",
        "outputId": "6ff5dc82-6e31-4a8f-9414-fdd6e908132c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 lines of test_pairs.txt:\n",
            "05006_00.jpg 11001_00.jpg\n",
            "02532_00.jpg 14096_00.jpg\n",
            "03921_00.jpg 08015_00.jpg\n",
            "12419_00.jpg 01944_00.jpg\n",
            "12562_00.jpg 14025_00.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATA SET CLASS"
      ],
      "metadata": {
        "id": "LjEuL4JPf_PN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VITONDataset(Dataset):\n",
        "    def __init__(self, root_dir, is_train=True, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.is_train = is_train\n",
        "        self.transform = transform\n",
        "        self.split = 'train' if is_train else 'test'\n",
        "\n",
        "        # Get all image files from the directory\n",
        "        self.image_dir = os.path.join(root_dir, self.split, 'image')\n",
        "        self.cloth_dir = os.path.join(root_dir, self.split, 'cloth')\n",
        "\n",
        "        # Get list of files\n",
        "        self.image_files = sorted(os.listdir(self.image_dir))\n",
        "        self.cloth_files = sorted(os.listdir(self.cloth_dir))\n",
        "\n",
        "        # Take only first 100 files for testing\n",
        "        self.image_files = self.image_files[:100]\n",
        "        self.cloth_files = self.cloth_files[:100]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get file names\n",
        "        image_name = self.image_files[idx]\n",
        "        cloth_name = self.cloth_files[idx]  # For simplicity, using same index\n",
        "\n",
        "        # Construct full paths\n",
        "        image_path = os.path.join(self.image_dir, image_name)\n",
        "        cloth_path = os.path.join(self.cloth_dir, cloth_name)\n",
        "\n",
        "        # Load images\n",
        "        try:\n",
        "            person_img = Image.open(image_path).convert('RGB')\n",
        "            cloth_img = Image.open(cloth_path).convert('RGB')\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading images: {e}\")\n",
        "            print(f\"Image path: {image_path}\")\n",
        "            print(f\"Cloth path: {cloth_path}\")\n",
        "            raise e\n",
        "\n",
        "        if self.transform:\n",
        "            person_img = self.transform(person_img)\n",
        "            cloth_img = self.transform(cloth_img)\n",
        "\n",
        "        return {\n",
        "            'person': person_img,\n",
        "            'cloth': cloth_img,\n",
        "            'person_name': image_name,\n",
        "            'cloth_name': cloth_name\n",
        "        }\n",
        "\n",
        "# Define transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((512, 384)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])"
      ],
      "metadata": {
        "id": "ElNOsxQhgBcW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize dataset with a try-except block\n",
        "try:\n",
        "    # Create dataset instance\n",
        "    dataset_path = '/content/drive/MyDrive/VITON_HD_Project/dataset'\n",
        "    train_dataset = VITONDataset(dataset_path, is_train=True, transform=transform)\n",
        "\n",
        "    print(f\"Dataset size: {len(train_dataset)}\")\n",
        "\n",
        "    # Create dataloader\n",
        "    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "    # Try loading one batch\n",
        "    for batch in train_loader:\n",
        "        print(\"\\nSuccessfully loaded batch:\")\n",
        "        print(\"Person image shape:\", batch['person'].shape)\n",
        "        print(\"Cloth image shape:\", batch['cloth'].shape)\n",
        "        print(\"Person filename:\", batch['person_name'])\n",
        "        print(\"Cloth filename:\", batch['cloth_name'])\n",
        "        break\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error occurred: {str(e)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtIwNqoogHs3",
        "outputId": "7d5e195b-9255-4285-f4ef-6510d27f2941"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset size: 100\n",
            "\n",
            "Successfully loaded batch:\n",
            "Person image shape: torch.Size([1, 3, 512, 384])\n",
            "Cloth image shape: torch.Size([1, 3, 512, 384])\n",
            "Person filename: ['00054_00.jpg']\n",
            "Cloth filename: ['00054_00.jpg']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    train_image_dir = os.path.join(base_path, 'train', 'image')\n",
        "    train_cloth_dir = os.path.join(base_path, 'train', 'cloth')\n",
        "\n",
        "    print(\"Files in train/image directory:\")\n",
        "    print(os.listdir(train_image_dir)[:5])  # First 5 files\n",
        "\n",
        "    print(\"\\nFiles in train/cloth directory:\")\n",
        "    print(os.listdir(train_cloth_dir)[:5])  # First 5 files\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error checking directories: {str(e)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKrcG4qzgQos",
        "outputId": "2a745adf-0cb9-47e5-f2e5-b3b280282897"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files in train/image directory:\n",
            "['00000_00.jpg', '00001_00.jpg', '00002_00.jpg', '00003_00.jpg', '00005_00.jpg']\n",
            "\n",
            "Files in train/cloth directory:\n",
            "['00000_00.jpg', '00001_00.jpg', '00002_00.jpg', '00003_00.jpg', '00005_00.jpg']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "import os\n",
        "import psutil\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "def clear_memory():\n",
        "    \"\"\"Clear GPU and RAM memory\"\"\"\n",
        "    # Clear GPU\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "    # Clear RAM\n",
        "    gc.collect()\n",
        "\n",
        "    # Delete existing models\n",
        "    try:\n",
        "        del model\n",
        "        del optimizer\n",
        "        del train_loader\n",
        "        del test_loader\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # Print memory status\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"GPU Memory allocated: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
        "        print(f\"GPU Memory cached: {torch.cuda.memory_reserved()/1e9:.2f} GB\")\n",
        "\n",
        "    process = psutil.Process(os.getpid())\n",
        "    print(f\"RAM Used: {process.memory_info().rss/1e9:.2f} GB\")\n",
        "\n",
        "# Set environment variable for memory management\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "\n",
        "# Clear everything\n",
        "clear_memory()\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Set random seeds\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2ZxnwSbgdO9",
        "outputId": "ad78240b-9729-4da2-e7d8-9fcd246c8996"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Memory allocated: 0.00 GB\n",
            "GPU Memory cached: 0.00 GB\n",
            "RAM Used: 0.59 GB\n",
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ENHANCED DATASET"
      ],
      "metadata": {
        "id": "diiCQUDSgkap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ImprovedVITONDataset(Dataset):\n",
        "    def __init__(self, root_dir, is_train=True, max_samples=2000):\n",
        "        self.root_dir = root_dir\n",
        "        self.is_train = is_train\n",
        "        self.split = 'train' if is_train else 'test'\n",
        "\n",
        "        # Get all directories\n",
        "        self.image_dir = os.path.join(root_dir, self.split, 'image')\n",
        "        self.cloth_dir = os.path.join(root_dir, self.split, 'cloth')\n",
        "        self.segment_dir = os.path.join(root_dir, self.split, 'image-parse')\n",
        "\n",
        "        # Get files with limit\n",
        "        self.image_files = sorted(os.listdir(self.image_dir))[:max_samples]\n",
        "        self.cloth_files = sorted(os.listdir(self.cloth_dir))[:max_samples]\n",
        "\n",
        "        # Advanced augmentation\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((512, 384)),\n",
        "            transforms.RandomHorizontalFlip(p=0.3),\n",
        "            transforms.ColorJitter(\n",
        "                brightness=0.2,\n",
        "                contrast=0.2,\n",
        "                saturation=0.2,\n",
        "                hue=0.1\n",
        "            ),\n",
        "            transforms.RandomAffine(\n",
        "                degrees=5,\n",
        "                translate=(0.05, 0.05),\n",
        "                scale=(0.95, 1.05)\n",
        "            ),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        try:\n",
        "            # Load images\n",
        "            image_name = self.image_files[idx]\n",
        "            cloth_name = self.cloth_files[idx]\n",
        "\n",
        "            image_path = os.path.join(self.image_dir, image_name)\n",
        "            cloth_path = os.path.join(self.cloth_dir, cloth_name)\n",
        "\n",
        "            person_img = Image.open(image_path).convert('RGB')\n",
        "            cloth_img = Image.open(cloth_path).convert('RGB')\n",
        "\n",
        "            # Transform images\n",
        "            if self.transform:\n",
        "                person_img = self.transform(person_img)\n",
        "                cloth_img = self.transform(cloth_img)\n",
        "\n",
        "            return {\n",
        "                'person': person_img,\n",
        "                'cloth': cloth_img,\n",
        "                'person_name': image_name,\n",
        "                'cloth_name': cloth_name\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {image_name}: {str(e)}\")\n",
        "            return self.__getitem__((idx + 1) % self.__len__())"
      ],
      "metadata": {
        "id": "Ru7bwiKRgmnb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MODEL ARCHITECTURE"
      ],
      "metadata": {
        "id": "VFlTVnCXgp_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EnhancedVITONModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Feature Extraction\n",
        "        vgg = models.vgg19(pretrained=True)\n",
        "        self.feature_extraction = nn.Sequential(*list(vgg.features.children())[:18])\n",
        "        for param in self.feature_extraction.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Attention Module\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.InstanceNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 1, kernel_size=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # Enhanced Warping Module\n",
        "        self.warping = nn.Sequential(\n",
        "            nn.Conv2d(512, 256, 3, padding=1),\n",
        "            nn.InstanceNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 128, 3, padding=1),\n",
        "            nn.InstanceNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 64, 3, padding=1),\n",
        "            nn.InstanceNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 2, 3, padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        # Generator Encoder\n",
        "        self.enc1 = nn.Sequential(\n",
        "                  nn.Conv2d(6, 64, 3, padding=1),\n",
        "                  nn.InstanceNorm2d(64),\n",
        "                  nn.ReLU(inplace=True)\n",
        "              )\n",
        "        self.enc2 = nn.Sequential(\n",
        "                  nn.Conv2d(64, 128, 3, padding=1),\n",
        "                  nn.InstanceNorm2d(128),\n",
        "                  nn.ReLU(inplace=True),\n",
        "                  nn.MaxPool2d(2)\n",
        "              )\n",
        "        self.enc3 = nn.Sequential(\n",
        "                  nn.Conv2d(128, 256, 3, padding=1),\n",
        "                  nn.InstanceNorm2d(256),\n",
        "                  nn.ReLU(inplace=True),\n",
        "                  nn.MaxPool2d(2)\n",
        "              )\n",
        "        self.enc4 = nn.Sequential(\n",
        "                  nn.Conv2d(256, 512, 3, padding=1),\n",
        "                  nn.InstanceNorm2d(512),\n",
        "                  nn.ReLU(inplace=True)\n",
        "              )\n",
        "\n",
        "\n",
        "        # Generator Decoder with correct dimensions\n",
        "        self.dec1 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(512, 256, 4, stride=2, padding=1),\n",
        "            nn.InstanceNorm2d(256),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        # Additional upsampling layer for d1 to match e3\n",
        "        self.upsample_d1 = nn.Upsample(\n",
        "                size=(128, 96),  # Match e3 size\n",
        "                mode='bilinear',\n",
        "                align_corners=True\n",
        "            )\n",
        "        self.upsample_d2 = nn.Upsample(\n",
        "                size=(256, 192),  # Match e2 size\n",
        "                mode='bilinear',\n",
        "                align_corners=True\n",
        "            )\n",
        "        self.upsample_d3 = nn.Upsample(\n",
        "                size=(512, 384),  # Match e1 size\n",
        "                mode='bilinear',\n",
        "                align_corners=True\n",
        "            )\n",
        "\n",
        "        self.dec2 = nn.Sequential(\n",
        "                nn.Conv2d(512, 256, 3, padding=1),\n",
        "                nn.InstanceNorm2d(256),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(256, 128, 3, padding=1)\n",
        "            )\n",
        "\n",
        "        self.dec3 = nn.Sequential(\n",
        "                nn.Conv2d(256, 128, 3, padding=1),\n",
        "                nn.InstanceNorm2d(128),\n",
        "                nn.ReLU(inplace=True)\n",
        "            )\n",
        "\n",
        "        self.dec4 = nn.Sequential(\n",
        "                nn.Conv2d(192, 64, 3, padding=1),\n",
        "                nn.InstanceNorm2d(64),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(64, 3, 3, padding=1),\n",
        "                nn.Tanh()\n",
        "            )\n",
        "\n",
        "\n",
        "\n",
        "        # Upsampling for flow field\n",
        "        self.upsample = nn.Upsample(\n",
        "            size=(512, 384),\n",
        "            mode='bilinear',\n",
        "            align_corners=True\n",
        "        )\n",
        "\n",
        "    def forward(self, person_img, cloth_img):\n",
        "        # Extract features\n",
        "        with torch.no_grad():\n",
        "            person_features = self.feature_extraction(person_img)\n",
        "            cloth_features = self.feature_extraction(cloth_img)\n",
        "\n",
        "        # Apply attention\n",
        "        attention_mask = self.attention(cloth_features)\n",
        "        cloth_features = cloth_features * attention_mask\n",
        "\n",
        "        # Generate flow field\n",
        "        combined_features = torch.cat([person_features, cloth_features], dim=1)\n",
        "        flow_field = self.warping(combined_features)\n",
        "        flow_field = self.upsample(flow_field)\n",
        "\n",
        "        # Generate sampling grid\n",
        "        batch_size = person_img.size(0)\n",
        "        grid = F.affine_grid(\n",
        "            torch.eye(2, 3).unsqueeze(0).repeat(batch_size, 1, 1).to(person_img.device),\n",
        "            size=person_img.size(),\n",
        "            align_corners=True\n",
        "        )\n",
        "\n",
        "        # Warp cloth\n",
        "        warped_cloth = F.grid_sample(\n",
        "            cloth_img,\n",
        "            grid + flow_field.permute(0, 2, 3, 1),\n",
        "            mode='bilinear',\n",
        "            padding_mode='border',\n",
        "            align_corners=True\n",
        "        )\n",
        "        x = torch.cat([person_img, warped_cloth], dim=1)\n",
        "        print(\"\\nEncoder shapes:\")\n",
        "        e1 = self.enc1(x)\n",
        "        print(f\"e1: {e1.shape}\")\n",
        "\n",
        "        e2 = self.enc2(e1)\n",
        "        print(f\"e2: {e2.shape}\")\n",
        "\n",
        "        e3 = self.enc3(e2)\n",
        "        print(f\"e3: {e3.shape}\")\n",
        "\n",
        "        e4 = self.enc4(e3)\n",
        "        print(f\"e4: {e4.shape}\")\n",
        "\n",
        "        # Decoder with debugging\n",
        "        print(\"\\nDecoder shapes:\")\n",
        "        # First decoder block\n",
        "        d1 = self.dec1(e4)\n",
        "        print(f\"d1 after ConvTranspose: {d1.shape}\")\n",
        "\n",
        "        d1 = self.upsample_d1(d1)\n",
        "        print(f\"d1 after upsample: {d1.shape}\")\n",
        "        print(f\"e3 for concatenation: {e3.shape}\")\n",
        "\n",
        "        d1_cat = torch.cat([d1, e3], dim=1)\n",
        "        print(f\"d1_cat: {d1_cat.shape}\")\n",
        "\n",
        "        # Second decoder block\n",
        "        d2 = self.dec2(d1_cat)\n",
        "        print(f\"d2 before upsample: {d2.shape}\")\n",
        "\n",
        "        d2 = self.upsample_d2(d2)\n",
        "        print(f\"d2 after upsample: {d2.shape}\")\n",
        "        print(f\"e2 for concatenation: {e2.shape}\")\n",
        "\n",
        "        d2_cat = torch.cat([d2, e2], dim=1)\n",
        "        print(f\"d2_cat: {d2_cat.shape}\")\n",
        "\n",
        "        # Third decoder block\n",
        "        d3 = self.dec3(d2_cat)\n",
        "        print(f\"d3 before upsample: {d3.shape}\")\n",
        "\n",
        "        d3 = self.upsample_d3(d3)\n",
        "        print(f\"d3 after upsample: {d3.shape}\")\n",
        "        print(f\"e1 for concatenation: {e1.shape}\")\n",
        "\n",
        "        d3_cat = torch.cat([d3, e1], dim=1)\n",
        "        print(f\"d3_cat: {d3_cat.shape}\")\n",
        "\n",
        "        # Final output\n",
        "        output = self.dec4(d3_cat)\n",
        "        print(f\"\\nFinal output: {output.shape}\")\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "    def get_warped_cloth(self, person_img, cloth_img):\n",
        "        \"\"\"Helper function to visualize intermediate results\"\"\"\n",
        "        with torch.no_grad():\n",
        "            # Extract features\n",
        "            person_features = self.feature_extraction(person_img)\n",
        "            cloth_features = self.feature_extraction(cloth_img)\n",
        "\n",
        "            # Apply attention\n",
        "            attention_mask = self.attention(cloth_features)\n",
        "            cloth_features = cloth_features * attention_mask\n",
        "\n",
        "            # Generate flow field\n",
        "            combined_features = torch.cat([person_features, cloth_features], dim=1)\n",
        "            flow_field = self.warping(combined_features)\n",
        "            flow_field = self.upsample(flow_field)\n",
        "\n",
        "            # Generate grid and warp\n",
        "            batch_size = person_img.size(0)\n",
        "            grid = F.affine_grid(\n",
        "                torch.eye(2, 3).unsqueeze(0).repeat(batch_size, 1, 1).to(person_img.device),\n",
        "                size=person_img.size(),\n",
        "                align_corners=True\n",
        "            )\n",
        "\n",
        "            warped_cloth = F.grid_sample(\n",
        "                cloth_img,\n",
        "                grid + flow_field.permute(0, 2, 3, 1),\n",
        "                mode='bilinear',\n",
        "                padding_mode='border',\n",
        "                align_corners=True\n",
        "            )\n",
        "\n",
        "            return warped_cloth, attention_mask\n",
        "\n",
        "def test_enhanced_model():\n",
        "    model = EnhancedVITONModel().to(device)\n",
        "    person = torch.randn(4, 3, 512, 384).to(device)\n",
        "    cloth = torch.randn(4, 3, 512, 384).to(device)\n",
        "\n",
        "    try:\n",
        "        # Add shape printing for debugging\n",
        "        output = model(person, cloth)\n",
        "        warped_cloth, attention = model.get_warped_cloth(person, cloth)\n",
        "\n",
        "        print(\"Shapes:\")\n",
        "        print(f\"Output: {output.shape}\")\n",
        "        print(f\"Warped cloth: {warped_cloth.shape}\")\n",
        "        print(f\"Attention mask: {attention.shape}\")\n",
        "        print(\"\\nModel test successful!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {str(e)}\")\n",
        "\n",
        "# Test the model\n",
        "test_enhanced_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sh_mEGN2gssA",
        "outputId": "e76ce646-e299-4f1a-91a5-1f20f7c9ea80"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n",
            "Shapes:\n",
            "Output: torch.Size([4, 3, 512, 384])\n",
            "Warped cloth: torch.Size([4, 3, 512, 384])\n",
            "Attention mask: torch.Size([4, 1, 128, 96])\n",
            "\n",
            "Model test successful!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LOSS FUNCTION"
      ],
      "metadata": {
        "id": "zN97n6GfhFT-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CompositeLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # VGG for perceptual loss\n",
        "        vgg = models.vgg19(pretrained=True).features[:29].eval()\n",
        "        for param in vgg.parameters():\n",
        "            param.requires_grad = False\n",
        "        self.vgg = vgg.to(device)\n",
        "\n",
        "        # Loss components\n",
        "        self.l1_loss = nn.L1Loss()\n",
        "        self.mse_loss = nn.MSELoss()\n",
        "\n",
        "    def perceptual_loss(self, x, target):\n",
        "        \"\"\"VGG based perceptual loss\"\"\"\n",
        "        vgg_x = self.vgg(x)\n",
        "        vgg_target = self.vgg(target)\n",
        "        return F.mse_loss(vgg_x, vgg_target)\n",
        "\n",
        "    def style_loss(self, x, target):\n",
        "        \"\"\"Gram matrix based style loss\"\"\"\n",
        "        def gram_matrix(feat):\n",
        "            b, ch, h, w = feat.size()\n",
        "            feat = feat.view(b, ch, h * w)\n",
        "            gram = torch.bmm(feat, feat.transpose(1, 2))\n",
        "            return gram.div(ch * h * w)\n",
        "\n",
        "        x_gram = gram_matrix(self.vgg(x))\n",
        "        target_gram = gram_matrix(self.vgg(target))\n",
        "        return F.mse_loss(x_gram, target_gram)\n",
        "\n",
        "    def forward(self, output, target, warped_cloth=None):\n",
        "        # Calculate different loss components\n",
        "        l1 = self.l1_loss(output, target)\n",
        "        perceptual = self.perceptual_loss(output, target)\n",
        "        style = self.style_loss(output, target)\n",
        "\n",
        "        # Warping loss if available\n",
        "        warp_loss = torch.tensor(0.0).to(device)\n",
        "        if warped_cloth is not None:\n",
        "            warp_loss = self.l1_loss(warped_cloth, target)\n",
        "\n",
        "        # Combine losses with weights\n",
        "        total_loss = (0.5 * l1 +\n",
        "                     0.2 * perceptual +\n",
        "                     0.2 * style +\n",
        "                     0.1 * warp_loss)\n",
        "\n",
        "        return total_loss"
      ],
      "metadata": {
        "id": "i1URL0_Xg8vp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Functions"
      ],
      "metadata": {
        "id": "6QgG9PAbhIN4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, dataloader, criterion, optimizer, device, gradient_accumulation_steps):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    progress_bar = tqdm(dataloader, desc='Training')\n",
        "\n",
        "    for batch_idx, batch in enumerate(progress_bar):\n",
        "        # Load data\n",
        "        person_images = batch['person'].to(device)\n",
        "        cloth_images = batch['cloth'].to(device)\n",
        "\n",
        "        # Clear gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        try:\n",
        "            # Forward pass\n",
        "            outputs = model(person_images, cloth_images)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = criterion(outputs, person_images)\n",
        "\n",
        "            # Normalize loss for gradient accumulation\n",
        "            loss = loss / gradient_accumulation_steps\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Update weights if needed\n",
        "            if (batch_idx + 1) % gradient_accumulation_steps == 0:\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            # Update metrics\n",
        "            total_loss += loss.item() * gradient_accumulation_steps\n",
        "            progress_bar.set_postfix({'loss': loss.item() * gradient_accumulation_steps})\n",
        "\n",
        "            # Clear memory periodically\n",
        "            if batch_idx % 10 == 0:\n",
        "                clear_memory()\n",
        "\n",
        "        except RuntimeError as e:\n",
        "            print(f\"Error in batch {batch_idx}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "def evaluate_model(model, test_loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_psnr = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            person_images = batch['person'].to(device)\n",
        "            cloth_images = batch['cloth'].to(device)\n",
        "\n",
        "            try:\n",
        "                outputs = model(person_images, cloth_images)\n",
        "                loss = criterion(outputs, person_images)\n",
        "\n",
        "                # Calculate PSNR\n",
        "                mse = F.mse_loss(outputs, person_images)\n",
        "                psnr = 10 * torch.log10(1 / mse)\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                total_psnr += psnr.item()\n",
        "\n",
        "            except RuntimeError as e:\n",
        "                print(f\"Error during evaluation: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "    return total_loss / len(test_loader), total_psnr / len(test_loader)"
      ],
      "metadata": {
        "id": "yNnDxbfahNlr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRAINING AND EVALUATION"
      ],
      "metadata": {
        "id": "I7q3dwiQhV5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear memory before starting\n",
        "clear_memory()\n",
        "\n",
        "# Initialize model\n",
        "model = EnhancedVITONModel().to(device)\n",
        "\n",
        "# Training parameters\n",
        "num_epochs = 30\n",
        "batch_size = 4\n",
        "gradient_accumulation_steps = 2\n",
        "learning_rate = 0.0001\n",
        "\n",
        "# Initialize optimizer and scheduler\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
        ")\n",
        "\n",
        "# Initialize loss function\n",
        "criterion = CompositeLoss().to(device)\n",
        "\n",
        "# Create dataloaders\n",
        "dataset_path = \"/content/drive/MyDrive/VITON_HD_Project/dataset\"\n",
        "train_dataset = ImprovedVITONDataset(dataset_path, is_train=True, max_samples=2000)\n",
        "test_dataset = ImprovedVITONDataset(dataset_path, is_train=False, max_samples=200)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "# Training metrics tracking\n",
        "class MetricsTracker:\n",
        "    def __init__(self):\n",
        "        self.train_losses = []\n",
        "        self.test_losses = []\n",
        "        self.psnr_scores = []\n",
        "\n",
        "    def update(self, train_loss, test_loss, psnr):\n",
        "        self.train_losses.append(train_loss)\n",
        "        self.test_losses.append(test_loss)\n",
        "        self.psnr_scores.append(psnr)\n",
        "\n",
        "    def plot_metrics(self):\n",
        "        plt.figure(figsize=(15, 5))\n",
        "\n",
        "        plt.subplot(1, 3, 1)\n",
        "        plt.plot(self.train_losses)\n",
        "        plt.title('Training Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "\n",
        "        plt.subplot(1, 3, 2)\n",
        "        plt.plot(self.test_losses)\n",
        "        plt.title('Test Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "\n",
        "        plt.subplot(1, 3, 3)\n",
        "        plt.plot(self.psnr_scores)\n",
        "        plt.title('PSNR Score')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('PSNR (dB)')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# Initialize metrics tracker\n",
        "metrics = MetricsTracker()\n",
        "\n",
        "# Training loop\n",
        "best_psnr = 0\n",
        "for epoch in range(num_epochs):\n",
        "    print(f'\\nEpoch [{epoch + 1}/{num_epochs}]')\n",
        "\n",
        "    # Train\n",
        "    train_loss = train_epoch(\n",
        "        model, train_loader, criterion, optimizer, device, gradient_accumulation_steps\n",
        "    )\n",
        "\n",
        "    # Evaluate\n",
        "    test_loss, psnr = evaluate_model(model, test_loader, criterion, device)\n",
        "    print(f'Train Loss: {train_loss:.4f}')\n",
        "    print(f'Test Loss: {test_loss:.4f}')\n",
        "    print(f'PSNR: {psnr:.2f} dB')\n",
        "\n",
        "    # Update metrics\n",
        "    metrics.update(train_loss, test_loss, psnr)\n",
        "\n",
        "    # Update learning rate\n",
        "    scheduler.step(test_loss)\n",
        "\n",
        "    # Save best model\n",
        "    if psnr > best_psnr:\n",
        "        best_psnr = psnr\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'psnr': psnr,\n",
        "        }, f'/content/drive/MyDrive/VITON_HD_Project/models/best_model.pth')\n",
        "        print(f'New best model saved with PSNR: {psnr:.2f} dB')\n",
        "\n",
        "    # Regular checkpoint\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'psnr': psnr,\n",
        "        }, f'/content/drive/MyDrive/VITON_HD_Project/models/checkpoint_epoch_{epoch+1}.pth')\n",
        "\n",
        "    # Clear memory\n",
        "    clear_memory()\n",
        "\n",
        "# Plot final metrics\n",
        "metrics.plot_metrics()\n",
        "print(\"Training completed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAoAdV7uhZMK",
        "outputId": "d692351b-3a97-44d2-dc2a-d8fedd8bdb31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Memory allocated: 0.01 GB\n",
            "GPU Memory cached: 0.04 GB\n",
            "RAM Used: 0.90 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch [1/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/500 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   0%|          | 1/500 [00:02<23:48,  2.86s/it, loss=2.98]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Memory allocated: 0.17 GB\n",
            "GPU Memory cached: 0.26 GB\n",
            "RAM Used: 1.26 GB\n",
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   0%|          | 2/500 [00:04<18:04,  2.18s/it, loss=2.95]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   1%|          | 3/500 [00:06<15:53,  1.92s/it, loss=2.78]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   1%|          | 4/500 [00:07<14:56,  1.81s/it, loss=2.56]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   1%|          | 5/500 [00:09<14:23,  1.74s/it, loss=2.38]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   1%|          | 6/500 [00:11<14:02,  1.71s/it, loss=2.59]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   1%|         | 7/500 [00:12<13:46,  1.68s/it, loss=2.74]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   2%|         | 8/500 [00:14<13:39,  1.67s/it, loss=2.67]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   2%|         | 9/500 [00:15<13:33,  1.66s/it, loss=2.46]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   2%|         | 10/500 [00:17<13:29,  1.65s/it, loss=2.7]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   2%|         | 11/500 [00:19<14:19,  1.76s/it, loss=2.24]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Memory allocated: 0.23 GB\n",
            "GPU Memory cached: 0.32 GB\n",
            "RAM Used: 1.28 GB\n",
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   2%|         | 12/500 [00:21<14:01,  1.72s/it, loss=2.12]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   3%|         | 13/500 [00:22<13:51,  1.71s/it, loss=2.07]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   3%|         | 14/500 [00:24<13:42,  1.69s/it, loss=2.15]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   3%|         | 15/500 [00:26<13:35,  1.68s/it, loss=2.4]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   3%|         | 16/500 [00:27<13:32,  1.68s/it, loss=2.55]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   3%|         | 17/500 [00:29<13:29,  1.68s/it, loss=2.24]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   4%|         | 18/500 [00:31<13:26,  1.67s/it, loss=2.07]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   4%|         | 19/500 [00:32<13:24,  1.67s/it, loss=2.15]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   4%|         | 20/500 [00:34<13:24,  1.68s/it, loss=1.91]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   4%|         | 21/500 [00:36<14:24,  1.80s/it, loss=1.97]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Memory allocated: 0.23 GB\n",
            "GPU Memory cached: 0.28 GB\n",
            "RAM Used: 1.28 GB\n",
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   4%|         | 22/500 [00:38<14:04,  1.77s/it, loss=2.09]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   5%|         | 23/500 [00:40<13:52,  1.75s/it, loss=2.14]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   5%|         | 24/500 [00:41<13:44,  1.73s/it, loss=1.92]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   5%|         | 25/500 [00:43<13:36,  1.72s/it, loss=1.63]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   5%|         | 26/500 [00:45<13:31,  1.71s/it, loss=1.62]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   5%|         | 27/500 [00:46<13:27,  1.71s/it, loss=1.64]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   6%|         | 28/500 [00:48<13:25,  1.71s/it, loss=1.71]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   6%|         | 29/500 [00:50<13:22,  1.70s/it, loss=1.75]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   6%|         | 30/500 [03:07<5:31:15, 42.29s/it, loss=1.84]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   6%|         | 31/500 [03:09<3:56:03, 30.20s/it, loss=1.36]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Memory allocated: 0.23 GB\n",
            "GPU Memory cached: 0.30 GB\n",
            "RAM Used: 1.28 GB\n",
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   6%|         | 32/500 [03:10<2:48:43, 21.63s/it, loss=1.33]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   7%|         | 33/500 [03:12<2:01:41, 15.64s/it, loss=1.75]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   7%|         | 34/500 [03:14<1:28:49, 11.44s/it, loss=2.08]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   7%|         | 35/500 [03:15<1:05:51,  8.50s/it, loss=1.61]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   7%|         | 36/500 [03:17<49:50,  6.45s/it, loss=1.43]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   7%|         | 37/500 [03:19<38:39,  5.01s/it, loss=1.24]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   8%|         | 38/500 [03:20<30:53,  4.01s/it, loss=1.6]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   8%|         | 39/500 [03:22<25:23,  3.31s/it, loss=1.55]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   8%|         | 40/500 [03:24<21:35,  2.82s/it, loss=1.37]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   8%|         | 41/500 [03:26<19:43,  2.58s/it, loss=1.54]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Memory allocated: 0.23 GB\n",
            "GPU Memory cached: 0.32 GB\n",
            "RAM Used: 1.28 GB\n",
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   8%|         | 42/500 [03:27<17:34,  2.30s/it, loss=1.44]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   9%|         | 43/500 [03:29<16:08,  2.12s/it, loss=1.43]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   9%|         | 44/500 [03:31<15:07,  1.99s/it, loss=1.44]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   9%|         | 45/500 [03:32<14:24,  1.90s/it, loss=1.42]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   9%|         | 46/500 [03:34<13:55,  1.84s/it, loss=1.58]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   9%|         | 47/500 [03:36<13:34,  1.80s/it, loss=1.52]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  10%|         | 48/500 [03:37<13:19,  1.77s/it, loss=1.38]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  10%|         | 49/500 [03:39<13:08,  1.75s/it, loss=1.38]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  10%|         | 50/500 [03:41<12:59,  1.73s/it, loss=1.26]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  10%|         | 51/500 [03:43<13:42,  1.83s/it, loss=1.3]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Memory allocated: 0.23 GB\n",
            "GPU Memory cached: 0.32 GB\n",
            "RAM Used: 1.28 GB\n",
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  10%|         | 52/500 [03:45<13:21,  1.79s/it, loss=1.38]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  11%|         | 53/500 [03:46<13:09,  1.77s/it, loss=1.34]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  11%|         | 54/500 [03:48<12:59,  1.75s/it, loss=1.25]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  11%|         | 55/500 [03:50<12:51,  1.73s/it, loss=1.26]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  11%|         | 56/500 [03:51<12:43,  1.72s/it, loss=1.16]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  11%|        | 57/500 [03:53<12:35,  1.71s/it, loss=1.15]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  12%|        | 58/500 [03:55<12:30,  1.70s/it, loss=1.24]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  12%|        | 59/500 [03:56<12:25,  1.69s/it, loss=1.18]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  12%|        | 60/500 [03:58<12:21,  1.69s/it, loss=0.885]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  12%|        | 61/500 [04:00<13:12,  1.81s/it, loss=1.05]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Memory allocated: 0.23 GB\n",
            "GPU Memory cached: 0.30 GB\n",
            "RAM Used: 1.28 GB\n",
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  12%|        | 62/500 [04:02<12:54,  1.77s/it, loss=1.19]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  13%|        | 63/500 [04:04<12:39,  1.74s/it, loss=1.1]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  13%|        | 64/500 [04:05<12:26,  1.71s/it, loss=1.2]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  13%|        | 65/500 [04:07<12:17,  1.70s/it, loss=1.4]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  13%|        | 66/500 [04:09<12:11,  1.68s/it, loss=0.907]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  13%|        | 67/500 [04:10<12:06,  1.68s/it, loss=1.15]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  14%|        | 68/500 [04:12<12:01,  1.67s/it, loss=0.951]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  14%|        | 69/500 [04:14<11:58,  1.67s/it, loss=0.875]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  14%|        | 70/500 [04:15<11:56,  1.67s/it, loss=0.955]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  14%|        | 71/500 [04:17<12:39,  1.77s/it, loss=1.11]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Memory allocated: 0.23 GB\n",
            "GPU Memory cached: 0.34 GB\n",
            "RAM Used: 1.28 GB\n",
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  14%|        | 72/500 [04:19<12:23,  1.74s/it, loss=1.12]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  15%|        | 73/500 [04:21<12:12,  1.71s/it, loss=0.821]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  15%|        | 74/500 [04:22<12:03,  1.70s/it, loss=0.837]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  15%|        | 75/500 [04:24<11:56,  1.69s/it, loss=0.83]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  15%|        | 76/500 [04:25<11:51,  1.68s/it, loss=1.06]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  15%|        | 77/500 [04:27<11:47,  1.67s/it, loss=0.871]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  16%|        | 78/500 [04:29<11:44,  1.67s/it, loss=0.96]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  16%|        | 79/500 [04:30<11:42,  1.67s/it, loss=0.913]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  16%|        | 80/500 [04:32<11:40,  1.67s/it, loss=0.972]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  16%|        | 81/500 [04:34<12:22,  1.77s/it, loss=0.821]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Memory allocated: 0.23 GB\n",
            "GPU Memory cached: 0.30 GB\n",
            "RAM Used: 1.28 GB\n",
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  16%|        | 82/500 [04:36<12:05,  1.74s/it, loss=1]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  17%|        | 83/500 [04:37<11:55,  1.72s/it, loss=0.902]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  17%|        | 84/500 [04:39<11:45,  1.70s/it, loss=0.952]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  17%|        | 85/500 [04:41<11:40,  1.69s/it, loss=1.07]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  17%|        | 86/500 [04:42<11:36,  1.68s/it, loss=1.07]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  17%|        | 87/500 [04:44<11:33,  1.68s/it, loss=1.05]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  18%|        | 88/500 [04:46<11:30,  1.68s/it, loss=0.944]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  18%|        | 89/500 [04:47<11:28,  1.68s/it, loss=0.86]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  18%|        | 90/500 [04:49<11:26,  1.67s/it, loss=0.816]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  18%|        | 91/500 [04:51<12:07,  1.78s/it, loss=0.93]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Memory allocated: 0.23 GB\n",
            "GPU Memory cached: 0.34 GB\n",
            "RAM Used: 1.28 GB\n",
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  18%|        | 92/500 [04:53<11:51,  1.74s/it, loss=0.826]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  19%|        | 93/500 [04:55<11:41,  1.72s/it, loss=0.971]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  19%|        | 94/500 [04:56<11:34,  1.71s/it, loss=0.908]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  19%|        | 95/500 [04:58<11:28,  1.70s/it, loss=0.787]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  19%|        | 96/500 [05:00<11:24,  1.69s/it, loss=0.808]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  19%|        | 97/500 [05:01<11:20,  1.69s/it, loss=0.728]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  20%|        | 98/500 [05:03<11:17,  1.68s/it, loss=0.897]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  20%|        | 99/500 [05:05<11:13,  1.68s/it, loss=1.02]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  20%|        | 100/500 [05:06<11:11,  1.68s/it, loss=0.713]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  20%|        | 101/500 [05:08<11:50,  1.78s/it, loss=0.717]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Memory allocated: 0.23 GB\n",
            "GPU Memory cached: 0.30 GB\n",
            "RAM Used: 1.28 GB\n",
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  20%|        | 102/500 [05:10<11:35,  1.75s/it, loss=0.838]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  21%|        | 103/500 [05:12<11:25,  1.73s/it, loss=0.93]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  21%|        | 104/500 [05:13<11:18,  1.71s/it, loss=0.78]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  21%|        | 105/500 [05:15<11:11,  1.70s/it, loss=0.696]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  21%|        | 106/500 [05:17<11:05,  1.69s/it, loss=0.823]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  21%|       | 107/500 [05:18<11:01,  1.68s/it, loss=0.894]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  22%|       | 108/500 [05:20<10:59,  1.68s/it, loss=0.87]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  22%|       | 109/500 [05:22<10:55,  1.68s/it, loss=0.804]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  22%|       | 110/500 [05:23<10:52,  1.67s/it, loss=0.685]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  22%|       | 111/500 [05:25<11:37,  1.79s/it, loss=0.732]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Memory allocated: 0.23 GB\n",
            "GPU Memory cached: 0.34 GB\n",
            "RAM Used: 1.28 GB\n",
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  22%|       | 112/500 [05:27<11:20,  1.75s/it, loss=0.671]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  23%|       | 113/500 [05:29<11:09,  1.73s/it, loss=0.898]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  23%|       | 114/500 [05:30<11:00,  1.71s/it, loss=0.914]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  23%|       | 115/500 [05:32<10:53,  1.70s/it, loss=0.961]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  23%|       | 116/500 [05:34<10:47,  1.69s/it, loss=0.752]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  23%|       | 117/500 [05:35<10:42,  1.68s/it, loss=0.692]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  24%|       | 118/500 [05:37<10:39,  1.67s/it, loss=0.703]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  24%|       | 119/500 [05:39<10:37,  1.67s/it, loss=0.774]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  24%|       | 120/500 [05:40<10:35,  1.67s/it, loss=0.728]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  24%|       | 121/500 [05:42<11:14,  1.78s/it, loss=0.756]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Memory allocated: 0.23 GB\n",
            "GPU Memory cached: 0.30 GB\n",
            "RAM Used: 1.28 GB\n",
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  24%|       | 122/500 [05:44<10:58,  1.74s/it, loss=0.73]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  25%|       | 123/500 [05:46<10:48,  1.72s/it, loss=0.753]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  25%|       | 124/500 [05:47<10:39,  1.70s/it, loss=0.722]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  25%|       | 125/500 [05:49<10:33,  1.69s/it, loss=0.783]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  25%|       | 126/500 [05:51<10:28,  1.68s/it, loss=0.787]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  25%|       | 127/500 [05:52<10:26,  1.68s/it, loss=0.861]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  26%|       | 128/500 [05:54<10:23,  1.68s/it, loss=0.753]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  26%|       | 129/500 [05:56<10:21,  1.68s/it, loss=0.687]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  26%|       | 130/500 [05:57<10:19,  1.67s/it, loss=0.596]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  26%|       | 131/500 [05:59<10:55,  1.78s/it, loss=0.936]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Memory allocated: 0.23 GB\n",
            "GPU Memory cached: 0.34 GB\n",
            "RAM Used: 1.28 GB\n",
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  26%|       | 132/500 [06:01<10:40,  1.74s/it, loss=0.737]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  27%|       | 133/500 [06:03<10:31,  1.72s/it, loss=0.627]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  27%|       | 134/500 [06:04<10:24,  1.71s/it, loss=0.691]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  27%|       | 135/500 [06:06<10:17,  1.69s/it, loss=0.641]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  27%|       | 136/500 [06:08<10:14,  1.69s/it, loss=0.687]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  27%|       | 137/500 [06:09<10:11,  1.68s/it, loss=0.593]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  28%|       | 138/500 [06:11<10:08,  1.68s/it, loss=0.661]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  28%|       | 139/500 [06:13<10:05,  1.68s/it, loss=0.552]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  28%|       | 140/500 [06:14<10:02,  1.67s/it, loss=0.648]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  28%|       | 141/500 [06:16<10:38,  1.78s/it, loss=0.814]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Memory allocated: 0.23 GB\n",
            "GPU Memory cached: 0.30 GB\n",
            "RAM Used: 1.28 GB\n",
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  28%|       | 142/500 [06:18<10:24,  1.74s/it, loss=0.554]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  29%|       | 143/500 [06:20<10:14,  1.72s/it, loss=0.656]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  29%|       | 144/500 [06:21<10:08,  1.71s/it, loss=0.814]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  29%|       | 145/500 [06:23<10:03,  1.70s/it, loss=0.67]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  29%|       | 146/500 [06:25<09:58,  1.69s/it, loss=0.693]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  29%|       | 147/500 [06:26<09:53,  1.68s/it, loss=0.741]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  30%|       | 148/500 [06:28<09:50,  1.68s/it, loss=0.581]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoder shapes:\n",
            "e1: torch.Size([4, 64, 512, 384])\n",
            "e2: torch.Size([4, 128, 256, 192])\n",
            "e3: torch.Size([4, 256, 128, 96])\n",
            "e4: torch.Size([4, 512, 128, 96])\n",
            "\n",
            "Decoder shapes:\n",
            "d1 after ConvTranspose: torch.Size([4, 256, 256, 192])\n",
            "d1 after upsample: torch.Size([4, 256, 128, 96])\n",
            "e3 for concatenation: torch.Size([4, 256, 128, 96])\n",
            "d1_cat: torch.Size([4, 512, 128, 96])\n",
            "d2 before upsample: torch.Size([4, 128, 128, 96])\n",
            "d2 after upsample: torch.Size([4, 128, 256, 192])\n",
            "e2 for concatenation: torch.Size([4, 128, 256, 192])\n",
            "d2_cat: torch.Size([4, 256, 256, 192])\n",
            "d3 before upsample: torch.Size([4, 128, 256, 192])\n",
            "d3 after upsample: torch.Size([4, 128, 512, 384])\n",
            "e1 for concatenation: torch.Size([4, 64, 512, 384])\n",
            "d3_cat: torch.Size([4, 192, 512, 384])\n",
            "\n",
            "Final output: torch.Size([4, 3, 512, 384])\n"
          ]
        }
      ]
    }
  ]
}